{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f1ff5b-1f36-4869-9027-bc1213cf7424",
   "metadata": {},
   "source": [
    "# Set up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60e735-5beb-41a0-bd49-0139055a01f1",
   "metadata": {},
   "source": [
    "## Config items\n",
    "Variables that need to be configured according to user requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4df2dc29-d55f-4cc2-a7b5-59cb115c5260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP Project ID.  If not set `gcloud config get project` will be used\n",
    "PROJECT_ID=\"\"\n",
    "# GCP Region\n",
    "GCP_REGION=\"us-central1\"\n",
    "# Categories or drawings that we want to recognise or classify\n",
    "CATEGORIES_TO_PROCESS=[\"basketball\", \"cat\", \"owl\", \"airplane\", \"bucket\", \"diamond\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c2fe1-2d3c-4b9d-94fb-56f227b10837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "# Import required libraries\n",
    "import kfp\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import pipeline, component, Dataset, Input, Output\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "# Environment config\n",
    "BUCKET_NAME=\"\"\n",
    "IMAGES_BUCKET=\"images\"\n",
    "PIPELINE_ROOT=\"\"\n",
    "SAVED_IMAGES_FILE_NAME_EXT=\".txt\"\n",
    "SAVED_IMAGES_FILE_NAME_SUFFIX=\"saved_images\"\n",
    "SAVED_IMAGES_FILE_NAME=f\"{SAVED_IMAGES_FILE_NAME_SUFFIX}{SAVED_IMAGES_FILE_NAME_EXT}\"\n",
    "\n",
    "# Check that the environment has configured a GCP project\n",
    "def is_project_configured():\n",
    "    global PROJECT_ID\n",
    "    # Using gcloud command to get the current project\n",
    "    system_out = !gcloud config get project\n",
    "    project_id = system_out[0]\n",
    "    \n",
    "    # Check if the project is not set\n",
    "    check = not (project_id==\"\" or project_id.strip()==\"\")\n",
    "    return check, project_id\n",
    "\n",
    "# Ensure that a bucket exists.  \n",
    "def ensure_bucket_exists(bucket_name):\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    gcs_client = storage.Client()\n",
    "    bucket = gcs_client.bucket(bucket_name)\n",
    "    # Check if the GCS bucket already exists\n",
    "    if bucket.exists():\n",
    "        print(\"Bucket exists: \" + bucket_name)\n",
    "    else:\n",
    "        # Doesn't exist, so create one\n",
    "        print(\"Creating bucket: \" + bucket_name)\n",
    "        bucket.create()\n",
    "    \n",
    "    return\n",
    "\n",
    "# Setup environment config\n",
    "def env_setup():\n",
    "    from google.cloud import storage\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    import sys\n",
    "    \n",
    "    global BUCKET_NAME, PIPELINE_ROOT\n",
    "    # Check project ID is set\n",
    "    projectConfigured, project_id = is_project_configured()\n",
    "    if projectConfigured:\n",
    "        # Project ID is set\n",
    "        print(\"Using project: \" + project_id)\n",
    "        time_stamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        BUCKET_NAME=project_id + \"-qd\"\n",
    "        ensure_bucket_exists(BUCKET_NAME)\n",
    "        PIPELINE_ROOT = f\"{BUCKET_NAME}/{time_stamp}/pipeline-root/\"\n",
    "        print(\"Dataflow pipeline root is: \" + PIPELINE_ROOT)\n",
    "    else:\n",
    "        # Exit with error that project ID has not been configured\n",
    "        sys.exit(\"Set project using `gcloud config set project [project-id]`\")\n",
    "    return\n",
    "\n",
    "env_setup()\n",
    "\n",
    "# Enable autocomplete in the notebook\n",
    "def enable_notebook_autocompletion():\n",
    "    !pip install jupyter_contrib_nbextensions\n",
    "    !jupyter contrib nbextension install - user\n",
    "    from jedi import settings\n",
    "    settings.case_insensitive_completion = True\n",
    "    return\n",
    "# enable_notebook_autocompletion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d5c29-c7d1-485a-bf46-91b68658dc04",
   "metadata": {},
   "source": [
    "## Data Flow Pipeline to import & create image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa8d80-f705-415e-b54e-8a6f193470fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dataflow components for python\n",
    "!pip install apache-beam[interactive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "23ecdc6d-e3a9-46af-b18f-55e2385a9965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from io import BytesIO\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import numpy as np\n",
    "\n",
    "# Checks if the file already exists.\n",
    "# Creates an emtpy file if the file doesn't exist\n",
    "# file_name is a fully qualified file name, including the protocol\n",
    "# e.g. file_name = \"gs://some-gcs-bucket/folder/temp.txt\"\n",
    "def ensureFileExists(file_name):\n",
    "        print(\"checking if the file exists\")\n",
    "        # Check if the file exists\n",
    "        if not filesystems.FileSystems.exists(file_name):\n",
    "            print(\"file doesn't exit. creating one\")\n",
    "            # Create an empty file if it doesn't exist\n",
    "            original_file = filesystems.FileSystems.create(file_name, mime_type=\"text/txt\")\n",
    "            original_file.write(bytes(\"\", encoding='UTF-8'))\n",
    "            original_file.close()\n",
    "\n",
    "# Gets the numpy_bitmap file for each category\n",
    "# This file in the GCS bucket holds the images in that category\n",
    "class ProcessCategory(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        file_uri = \"gs://scratch-pad-kunall-qd/full/numpy_bitmap/\"+element+\".npy\"\n",
    "        # print(\"File URI is: \" + file_uri)\n",
    "        yield {\"category\": element, \"file_name\": file_uri}\n",
    "\n",
    "# Loads a required sample size of image data from the files for each category\n",
    "class LoadImageData(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        image_data_file = element[\"file_name\"]\n",
    "        # Read image data from the file into a numpy array\n",
    "        np_file = np.load(BytesIO(file_io.read_file_to_string(image_data_file, binary_mode=True))).astype(\"float32\")\n",
    "        # Randomly select X number of samples from the file\n",
    "        idx = np.random.randint(len(np_file), size=20)\n",
    "        # Resize the numpy array with the expected number of samples\n",
    "        for row in np_file[idx,:]:\n",
    "            # Reshape the array as 28 * 28 matrix\n",
    "            image_matrix = row.reshape(28, 28)\n",
    "            yield {\"category\": element[\"category\"], \"image\": image_matrix}\n",
    "\n",
    "# Save the individual image to GCS bucket\n",
    "class SaveImage(beam.DoFn):\n",
    "        def process(self, element):\n",
    "            category = element[\"category\"]\n",
    "            image = element[\"image\"]\n",
    "            outdir = \"gs://scratch-pad-kunall-qd/images/\" + category + \"/\"\n",
    "            file_name = uuid.uuid4().hex + '.png'\n",
    "            writer = filesystems.FileSystems.create(outdir + file_name )\n",
    "            writer.write(image)\n",
    "            writer.close()\n",
    "            yield outdir + file_name + \",\" + category\n",
    "\n",
    "# Create an image dataset that can be used to train image classification ML models\n",
    "def create_image_dataset():\n",
    "    saved_images_filename = \"gs://scratch-pad-kunall-qd/images/\" + SAVED_IMAGES_FILE_NAME\n",
    "    ensureFileExists(saved_images_filename)    \n",
    "    with beam.Pipeline() as p:\n",
    "        categories = (\n",
    "                p \n",
    "                | \"Read categories\" >> beam.Create(CATEGORIES_TO_PROCESS)\n",
    "                | \"Get file to category map\" >> beam.ParDo(ProcessCategory())\n",
    "                | \"Load Image Data File\" >> beam.ParDo(LoadImageData())\n",
    "                | \"Save Image\" >> beam.ParDo(SaveImage())\n",
    "                \n",
    "            )\n",
    "        original_file = (\n",
    "            p | \"Read existing saved images file\" >> beam.io.ReadFromText(saved_images_filename)\n",
    "        )\n",
    "        merged = (\n",
    "            (categories, original_file) | beam.Flatten() |\n",
    "             beam.io.WriteToText(\"gs://scratch-pad-kunall-qd/images/\" + SAVED_IMAGES_FILE_NAME_SUFFIX, \n",
    "                                 file_name_suffix=SAVED_IMAGES_FILE_NAME_EXT, \n",
    "                                 append_trailing_newlines=True, \n",
    "                                 shard_name_template='')\n",
    "        )\n",
    "        result = p.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_image_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "71de04fc-dfb5-4f45-90c6-d1674290c7ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25547/3750726422.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m def preprocess_drawings(\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdisplay_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categories\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCATEGORIES_TO_PROCESS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'project_id' is not defined"
     ]
    }
   ],
   "source": [
    "# @component(\n",
    "#     packages_to_install=[\n",
    "#         \"google-cloud-storage\",\n",
    "#         \"pandas\",\n",
    "#     ],\n",
    "#     base_image=\"python:3.9\",\n",
    "#     output_component_file=\"preprocess_drawings.yaml\"\n",
    "# )\n",
    "def preprocess_drawings(\n",
    "    project=project_id,\n",
    "    display_name=\"categories\",\n",
    "    categories=CATEGORIES_TO_PROCESS,\n",
    "    categories_dataset = Output[Dataset]        \n",
    "    ):\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    global BUCKET_NAME, IMAGES_BUCKET\n",
    "    ensure_bucket_exists(BUCKET_NAME)\n",
    "    for category in categories:\n",
    "        print(f\"Read from: gs://scratch-pad-kunall-qd/full/simplified/{category}.ndjson\")\n",
    "        with open(f'gs://scratch-pad-kunall-qd/full/simplified/{category}.ndjson', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        pd.DataFrame(data).head\n",
    "        print(f\"gs://{BUCKET_NAME}/{IMAGES_BUCKET}/{category}/key.png,{category}\")\n",
    "            \n",
    "    return\n",
    "preprocess_drawings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff1ed06-ad60-467e-b359-ebf3093aca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import pipeline, component, Dataset, Input, Output\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "\n",
    "project_id = \"scratch-pad-kunall\"\n",
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_NAME=\"gs://\" + project_id + \"-quickdraw-\" + TIMESTAMP\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/qd-pipeline-root/\"\n",
    "pipeline_root_path = PIPELINE_ROOT\n",
    "\n",
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"automl-image-training-v2\",\n",
    "    pipeline_root=pipeline_root_path,)\n",
    "def pipeline(project_id: str):\n",
    "    # The first step of your workflow is a dataset generator.\n",
    "    # This step takes a Google Cloud pipeline component, providing the necessary\n",
    "    # input arguments, and uses the Python variable `ds_op` to define its\n",
    "    # output. Note that here the `ds_op` only stores the definition of the\n",
    "    # output but not the actual returned object from the execution. The value\n",
    "    # of the object is not accessible at the dsl.pipeline level, and can only be\n",
    "    # retrieved by providing it as the input to a downstream component.\n",
    "    ds_op = gcc_aip.ImageDatasetCreateOp(\n",
    "        project=project_id,\n",
    "        display_name=\"flowers\",\n",
    "        gcs_source=\"gs://cloud-samples-data/vision/automl_classification/flowers/all_data_v2.csv\",\n",
    "        import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n",
    "    )\n",
    "\n",
    "    # The second step is a model training component. It takes the dataset\n",
    "    # outputted from the first step, supplies it as an input argument to the\n",
    "    # component (see `dataset=ds_op.outputs[\"dataset\"]`), and will put its\n",
    "    # outputs into `training_job_run_op`.\n",
    "    training_job_run_op = gcc_aip.AutoMLImageTrainingJobRunOp(\n",
    "        project=project_id,\n",
    "        display_name=\"train-iris-automl-mbsdk-1\",\n",
    "        prediction_type=\"classification\",\n",
    "        model_type=\"CLOUD\",\n",
    "        #base_model=None,\n",
    "        dataset=ds_op.outputs[\"dataset\"],\n",
    "        model_display_name=\"iris-classification-model-mbsdk\",\n",
    "        training_fraction_split=0.6,\n",
    "        validation_fraction_split=0.2,\n",
    "        test_fraction_split=0.2,\n",
    "        budget_milli_node_hours=8000,\n",
    "    )\n",
    "\n",
    "    # The third and fourth step are for deploying the model.\n",
    "    create_endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        display_name = \"create-endpoint\",\n",
    "    )\n",
    "\n",
    "    model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "        model=training_job_run_op.outputs[\"model\"],\n",
    "        endpoint=create_endpoint_op.outputs['endpoint'],\n",
    "        automatic_resources_min_replica_count=1,\n",
    "        automatic_resources_max_replica_count=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a84be8-b9f1-422a-83d8-5fb318685a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='image_classif_pipeline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b489fa-5bfa-488d-8cfb-b96d2e72b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"automl-image-training-v2\",\n",
    "    template_path=\"image_classif_pipeline.json\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'project_id': project_id\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m94"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
