{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f1ff5b-1f36-4869-9027-bc1213cf7424",
   "metadata": {},
   "source": [
    "# Set up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60e735-5beb-41a0-bd49-0139055a01f1",
   "metadata": {},
   "source": [
    "## Config items\n",
    "Variables that need to be configured according to user requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2dc29-d55f-4cc2-a7b5-59cb115c5260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP Project ID.  If not set `gcloud config get project` will be used\n",
    "PROJECT_ID=\"\"\n",
    "# GCP Region\n",
    "GCP_REGION=\"us-central1\"\n",
    "# Categories or drawings that we want to recognise or classify\n",
    "CATEGORIES_TO_PROCESS=[\"basketball\", \"cat\", \"owl\", \"airplane\", \"bucket\", \"diamond\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "888c2fe1-2d3c-4b9d-94fb-56f227b10837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project: scratch-pad-kunall\n",
      "Bucket exists: scratch-pad-kunall-qd\n",
      "Pipeline root is: scratch-pad-kunall-qd/20220712053849/pipeline-root/\n"
     ]
    }
   ],
   "source": [
    "# Basic setup\n",
    "# Import required libraries\n",
    "import kfp\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import pipeline, component, Dataset, Input, Output\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "# Environment config\n",
    "BUCKET_NAME=\"\"\n",
    "IMAGES_BUCKET=\"images\"\n",
    "PIPELINE_ROOT=\"\"\n",
    "SAVED_IMAGES_FILE_NAME_EXT=\"txt\"\n",
    "SAVED_IMAGES_FILE_NAME_SUFFIX=\"saved_images\"\n",
    "SAVED_IMAGES_FILE_NAME=f\"{SAVED_IMAGES_FILE_NAME_SUFFIX}.{SAVED_IMAGES_FILE_NAME_EXT}\"\n",
    "\n",
    "# Check that the environment has configured a GCP project\n",
    "def is_project_configured():\n",
    "    global PROJECT_ID\n",
    "    # Using gcloud command to get the current project\n",
    "    system_out = !gcloud config get project\n",
    "    project_id = system_out[0]\n",
    "    \n",
    "    # Check if the project is not set\n",
    "    check = not (project_id==\"\" or project_id.strip()==\"\")\n",
    "    return check\n",
    "\n",
    "# Ensure that a bucket exists.  \n",
    "def ensure_bucket_exists(bucket_name):\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    gcs_client = storage.Client()\n",
    "    bucket = gcs_client.bucket(bucket_name)\n",
    "    # Check if the GCS bucket already exists\n",
    "    if bucket.exists():\n",
    "        print(\"Bucket exists: \" + bucket_name)\n",
    "    else:\n",
    "        # Doesn't exist, so create one\n",
    "        print(\"Creating bucket: \" + bucket_name)\n",
    "        bucket.create()\n",
    "    \n",
    "    return\n",
    "\n",
    "def env_setup():\n",
    "    from google.cloud import storage\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    import sys\n",
    "    \n",
    "    global BUCKET_NAME, PIPELINE_ROOT\n",
    "    # Check project ID is set\n",
    "    if is_project_configured():\n",
    "        # Project ID is set\n",
    "        print(\"Using project: \" + project_id)\n",
    "        time_stamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        BUCKET_NAME=project_id + \"-qd\"\n",
    "        ensure_bucket_exists(BUCKET_NAME)\n",
    "        PIPELINE_ROOT = f\"{BUCKET_NAME}/{time_stamp}/pipeline-root/\"\n",
    "        print(\"Dataflow pipeline root is: \" + PIPELINE_ROOT)\n",
    "    else:\n",
    "        # Exit with error that project ID has not been configured\n",
    "        sys.exit(\"Set project using `gcloud config set project [project-id]`\")\n",
    "    return\n",
    "\n",
    "env_setup()\n",
    "\n",
    "# Enable autocomplete in the notebook\n",
    "def enable_notebook_autocompletion():\n",
    "    !pip install jupyter_contrib_nbextensions\n",
    "    !jupyter contrib nbextension install - user\n",
    "    from jedi import settings\n",
    "    settings.case_insensitive_completion = True\n",
    "    return\n",
    "enable_notebook_autocompletion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d5c29-c7d1-485a-bf46-91b68658dc04",
   "metadata": {},
   "source": [
    "## Data Flow Pipeline to import & create image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa8d80-f705-415e-b54e-8a6f193470fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dataflow components for python\n",
    "!pip install apache-beam[interactive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "0ab76575-f8e3-45ea-9daa-905f636ac93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking if the file exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse, json, io, base64, sys, uuid\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.io import filesystems\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "#from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "from PIL import Image #, ImageDraw\n",
    "import numpy as np\n",
    "#from typing import Iterable\n",
    "\n",
    "from io import BytesIO\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "# Reshape or transform the input array into a 28*28 matrix\n",
    "class EncodeFn(beam.DoFn):\n",
    "    def process(self, element, img_size=28, lw=3):\n",
    "        # import matplotlib.pyplot as plt\n",
    "        # %matplotlib inline  \n",
    "        # plt.imshow(element.reshape(28,28),cmap='gray', vmin=0, vmax=255) \n",
    "        image_data = Image.fromarray(element.reshape(28,28))\n",
    "        b = io.BytesIO()\n",
    "        image_data.convert('RGB').save(b, format='png')\n",
    "        # Save the byte value of the converted image as a value for key 'image'\n",
    "        yield {'image':b.getvalue()}\n",
    "\n",
    "# Save images to GCS (Google File System) bucket\n",
    "class WriteToSeparateFiles(beam.DoFn):\n",
    "    def __init__(self, outdir, category):\n",
    "        self.outdir = outdir\n",
    "        self.category = category\n",
    "        \n",
    "    def process(self, element):\n",
    "        file_name = uuid.uuid4().hex + '.png'\n",
    "        writer = filesystems.FileSystems.create(self.outdir + file_name )\n",
    "        writer.write(element['image'])\n",
    "        writer.close()\n",
    "        yield self.outdir + file_name + \",\" + self.category\n",
    "\n",
    "# class WriteSavedImages(beam.DoFn):\n",
    "#     def __init__(self, file_name):\n",
    "#         self.file_name = file_name\n",
    "        \n",
    "#     def process(self, element):\n",
    "#         original_content = \"\"\n",
    "#         with beam.io.gcsio.GcsIO().open(filename=self.file_name, mode=\"a\") as f:\n",
    "#             f.write(element.encode(\"utf-8\"))\n",
    "        # saved_images_file = filesystems.FileSystems.create(self.file_name, mime_type=\"text/txt\")\n",
    "        # saved_images_file.write(bytes(element, encoding='UTF-8'))\n",
    "        # saved_images_file.close()\n",
    "        \n",
    "# # def ensureFileExists(file_name) -> Iterable[str]:\n",
    "# class EnsureFileExists(beam.DoFn):\n",
    "#     def __init__(self, file_name):\n",
    "#         self.file_name = file_name\n",
    "\n",
    "#     def process(self, file_name):\n",
    "\n",
    "# The pipeline expects saved_images.txt file to exist\n",
    "# Checks the the file exists, and if not then creates it\n",
    "def ensureFileExists(file_name):\n",
    "        # print(\"checking if the file exists\")\n",
    "        if not filesystems.FileSystems.exists(file_name):\n",
    "            # print(\"file doesn't exit. creating one\")\n",
    "            # Create an empty file if it doesn't exist\n",
    "            original_file = filesystems.FileSystems.create(file_name, mime_type=\"text/txt\")\n",
    "            original_file.write(bytes(\"\", encoding='UTF-8'))\n",
    "            original_file.close()\n",
    "      \n",
    "# Run function to launch the dataflow pipeline        \n",
    "def run(argv=None):\n",
    "    global SAVED_IMAGES_FILE_NAME, SAVED_IMAGES_FILE_NAME_SUFFIX, SAVED_IMAGES_FILE_NAME_EXT\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--category',\n",
    "                      dest='category',\n",
    "                      required=True,\n",
    "                      help='Category of the drawing')\n",
    "    parser.add_argument('--sample-size',\n",
    "                      dest='size',\n",
    "                      nargs=\"?\",\n",
    "                      const=4000,\n",
    "                      help='Number of samples to be created')\n",
    "    parser.add_argument('--input',\n",
    "                      dest='input',\n",
    "                      required=True,\n",
    "                      help='GCS destination folder to read the ndjson file from (example: gs://BUCKET_NAME/path/to/')\n",
    "    parser.add_argument('--output',\n",
    "                      dest='output',\n",
    "                      required=True,\n",
    "                      help='GCS destination folder to save the images to (example: gs://BUCKET_NAME/path/to/images/')\n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "    # Read the category file from GCS bucket\n",
    "    np_file = np.load(BytesIO(file_io.read_file_to_string(known_args.input + known_args.category+\".npy\", binary_mode=True))).astype(\"float32\")\n",
    "    # Randomly select X number of samples.  Where X is the size input provided as an argument\n",
    "    idx = np.random.randint(len(np_file), size=int(known_args.size))\n",
    "    # Resize the numpy array with the expected number of samples\n",
    "    np_file = np_file[idx,:]\n",
    "    # print (np_file.shape)\n",
    "    # Use save_main_session option because EncodeFn relies on global context (Image module imported)\n",
    "    # Create the saved_images.txt file if one doesn't exist\n",
    "    ensureFileExists(known_args.output+SAVED_IMAGES_FILE_NAME)\n",
    "    \n",
    "    with beam.Pipeline(argv=pipeline_args) as p:\n",
    "        \n",
    "        images = (\n",
    "            p | \"Read Numpy file from GCS\" >> beam.Create(np_file) # TODO: create a seperate DoFn class for this\n",
    "              | \"Convert to images\" >> beam.ParDo(EncodeFn())\n",
    "              | \"Save images to GCS\" >> beam.ParDo(WriteToSeparateFiles(known_args.output+known_args.category+\"/\" , known_args.category))\n",
    "              # | beam.Map(print)\n",
    "              # | \"Write to file\" >> beam.io.WriteToText(known_args.output+\"saved_images\", file_name_suffix='.txt_temp',append_trailing_newlines=True,shard_name_template='')\n",
    "              # | \"List saved images\" >> beam.Map(print)\n",
    "        )\n",
    "        # ensure_saved_images_file_exists = (\n",
    "        #     p | \"Create saved images file if it doesn't exist\" >> beam.ParDo(EnsureFileExists(known_args.output+\"saved_images.txt\"))\n",
    "        # )\n",
    "        original_file = (\n",
    "            p | \"Read existing saved images file\" >> beam.io.ReadFromText(known_args.output+SAVED_IMAGES_FILE_NAME)\n",
    "            # p | \"Read existing saved images file\" >> beam.Create(beam.ParDo(EnsureFileExists(known_args.output+\"saved_images.txt\")))\n",
    "        )\n",
    "        \n",
    "        merged = (\n",
    "            (images,original_file) | beam.Flatten() |\n",
    "            # beam.ParDo(WriteSavedImages(known_args.output+\"saved_images\"))\n",
    "             beam.io.WriteToText(known_args.output+SAVED_IMAGES_FILE_NAME_SUFFIX, file_name_suffix=SAVED_IMAGES_FILE_NAME_EXT,append_trailing_newlines=True,shard_name_template='')\n",
    "        )\n",
    "\n",
    "    result = p.run()\n",
    "    # result.wait_until_finish()\n",
    "    \n",
    "# def image_map(image_file):\n",
    "#     name = known_args.output + known_args.category + \"/\" + image_file + \",\" + known_args.category\n",
    "#     print(name)\n",
    "#     return name\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # run([\"--input=gs://quickdraw_dataset/full/simplified/owl.ndjson\", \"--output=gs://scratch-pad-kunall-qd/images/\"])\n",
    "    run([\"--category=owl\", \"--sample-size=20\", \"--input=gs://scratch-pad-kunall-qd/full/numpy_bitmap/\", \"--output=gs://scratch-pad-kunall-qd/images/\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71de04fc-dfb5-4f45-90c6-d1674290c7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket exists: scratch-pad-kunall-qd\n",
      "Read from: gs://scratch-pad-kunall-qd/full/simplified/basketball.ndjson\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gs://scratch-pad-kunall-qd/full/simplified/basketball.ndjson'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31087/3750726422.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mpreprocess_drawings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_31087/3750726422.py\u001b[0m in \u001b[0;36mpreprocess_drawings\u001b[0;34m(project, display_name, categories, categories_dataset)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Read from: gs://scratch-pad-kunall-qd/full/simplified/{category}.ndjson\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'gs://scratch-pad-kunall-qd/full/simplified/{category}.ndjson'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gs://scratch-pad-kunall-qd/full/simplified/basketball.ndjson'"
     ]
    }
   ],
   "source": [
    "# @component(\n",
    "#     packages_to_install=[\n",
    "#         \"google-cloud-storage\",\n",
    "#         \"pandas\",\n",
    "#     ],\n",
    "#     base_image=\"python:3.9\",\n",
    "#     output_component_file=\"preprocess_drawings.yaml\"\n",
    "# )\n",
    "def preprocess_drawings(\n",
    "    project=project_id,\n",
    "    display_name=\"categories\",\n",
    "    categories=CATEGORIES_TO_PROCESS,\n",
    "    categories_dataset = Output[Dataset]        \n",
    "    ):\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    global BUCKET_NAME, IMAGES_BUCKET\n",
    "    ensure_bucket_exists(BUCKET_NAME)\n",
    "    for category in categories:\n",
    "        print(f\"Read from: gs://scratch-pad-kunall-qd/full/simplified/{category}.ndjson\")\n",
    "        with open(f'gs://scratch-pad-kunall-qd/full/simplified/{category}.ndjson', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        pd.DataFrame(data).head\n",
    "        print(f\"gs://{BUCKET_NAME}/{IMAGES_BUCKET}/{category}/key.png,{category}\")\n",
    "            \n",
    "    return\n",
    "preprocess_drawings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff1ed06-ad60-467e-b359-ebf3093aca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import pipeline, component, Dataset, Input, Output\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "\n",
    "project_id = \"scratch-pad-kunall\"\n",
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_NAME=\"gs://\" + project_id + \"-quickdraw-\" + TIMESTAMP\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/qd-pipeline-root/\"\n",
    "pipeline_root_path = PIPELINE_ROOT\n",
    "\n",
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"automl-image-training-v2\",\n",
    "    pipeline_root=pipeline_root_path,)\n",
    "def pipeline(project_id: str):\n",
    "    # The first step of your workflow is a dataset generator.\n",
    "    # This step takes a Google Cloud pipeline component, providing the necessary\n",
    "    # input arguments, and uses the Python variable `ds_op` to define its\n",
    "    # output. Note that here the `ds_op` only stores the definition of the\n",
    "    # output but not the actual returned object from the execution. The value\n",
    "    # of the object is not accessible at the dsl.pipeline level, and can only be\n",
    "    # retrieved by providing it as the input to a downstream component.\n",
    "    ds_op = gcc_aip.ImageDatasetCreateOp(\n",
    "        project=project_id,\n",
    "        display_name=\"flowers\",\n",
    "        gcs_source=\"gs://cloud-samples-data/vision/automl_classification/flowers/all_data_v2.csv\",\n",
    "        import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n",
    "    )\n",
    "\n",
    "    # The second step is a model training component. It takes the dataset\n",
    "    # outputted from the first step, supplies it as an input argument to the\n",
    "    # component (see `dataset=ds_op.outputs[\"dataset\"]`), and will put its\n",
    "    # outputs into `training_job_run_op`.\n",
    "    training_job_run_op = gcc_aip.AutoMLImageTrainingJobRunOp(\n",
    "        project=project_id,\n",
    "        display_name=\"train-iris-automl-mbsdk-1\",\n",
    "        prediction_type=\"classification\",\n",
    "        model_type=\"CLOUD\",\n",
    "        #base_model=None,\n",
    "        dataset=ds_op.outputs[\"dataset\"],\n",
    "        model_display_name=\"iris-classification-model-mbsdk\",\n",
    "        training_fraction_split=0.6,\n",
    "        validation_fraction_split=0.2,\n",
    "        test_fraction_split=0.2,\n",
    "        budget_milli_node_hours=8000,\n",
    "    )\n",
    "\n",
    "    # The third and fourth step are for deploying the model.\n",
    "    create_endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        display_name = \"create-endpoint\",\n",
    "    )\n",
    "\n",
    "    model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "        model=training_job_run_op.outputs[\"model\"],\n",
    "        endpoint=create_endpoint_op.outputs['endpoint'],\n",
    "        automatic_resources_min_replica_count=1,\n",
    "        automatic_resources_max_replica_count=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a84be8-b9f1-422a-83d8-5fb318685a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='image_classif_pipeline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b489fa-5bfa-488d-8cfb-b96d2e72b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"automl-image-training-v2\",\n",
    "    template_path=\"image_classif_pipeline.json\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'project_id': project_id\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
