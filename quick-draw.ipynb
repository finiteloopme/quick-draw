{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b733370-d1d1-46cf-ab9c-e3650031dfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./tmp’: File exists\n",
      "--2022-07-04 20:17:24--  https://raw.githubusercontent.com/finiteloopme/quick-draw/main/categories/requested.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 759 [text/plain]\n",
      "Saving to: ‘requested.txt.1’\n",
      "\n",
      "requested.txt.1     100%[===================>]     759  --.-KB/s    in 0s      \n",
      "\n",
      "2022-07-04 20:17:24 (57.9 MB/s) - ‘requested.txt.1’ saved [759/759]\n",
      "\n",
      "mkdir: cannot create directory ‘./data’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./tmp; cd ./tmp; wget \"https://raw.githubusercontent.com/finiteloopme/quick-draw/main/categories/requested.txt\"\n",
    "!mkdir ./data\n",
    "!mkdir ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e7fed7a-a41e-49a9-8143-c3cbe94e5228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_categories():\n",
    "    f_categories = open(\"./tmp/requested.txt\", \"r\")\n",
    "    # Read each category from a line in the file\n",
    "    categories = f_categories.readlines()\n",
    "    # Remove newline char and replace SPACE with UNDERSCORE\n",
    "    categories = [category.replace(\"\\n\", \"\").replace(\" \", \"_\") for category in categories]\n",
    "    f_categories.close()\n",
    "    return categories\n",
    "# pre_process_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20be4a27-9283-464d-b0de-a7e03c0d5ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "def download_data(categories):\n",
    "    base_url = \"https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/\"\n",
    "    for category in categories:\n",
    "        category_uri = category.replace(\"_\", \"%20\") + \".npy\"\n",
    "        request.urlretrieve(base_url+category_uri, \"./data/\"+category+\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b34296e-a811-4314-b4dc-d860ee6260e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(process_categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d42928c8-19c1-45b5-a566-6aa2b1bd19b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e222d2e-c507-43b3-925c-ab5c528e8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(root, vfold_ratio=0.2, max_items_per_class= 4000 ):\n",
    "    all_files = glob.glob(os.path.join(root, '*.npy'))\n",
    "\n",
    "    #initialize variables \n",
    "    x = np.empty([0, 784])\n",
    "    y = np.empty([0])\n",
    "    class_names = []\n",
    "\n",
    "    #load each data file \n",
    "    for idx, file in enumerate(all_files):\n",
    "        data = np.load(file)\n",
    "        data = data[0: max_items_per_class, :]\n",
    "        labels = np.full(data.shape[0], idx)\n",
    "\n",
    "        x = np.concatenate((x, data), axis=0)\n",
    "        y = np.append(y, labels)\n",
    "\n",
    "        class_name, ext = os.path.splitext(os.path.basename(file))\n",
    "        class_names.append(class_name)\n",
    "\n",
    "    data = None\n",
    "    labels = None\n",
    "    \n",
    "    #randomize the dataset \n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    x = x[permutation, :]\n",
    "    y = y[permutation]\n",
    "\n",
    "    #separate into training and testing \n",
    "    vfold_size = int(x.shape[0]/100*(vfold_ratio*100))\n",
    "\n",
    "    x_test = x[0:vfold_size, :]\n",
    "    y_test = y[0:vfold_size]\n",
    "\n",
    "    x_train = x[vfold_size:x.shape[0], :]\n",
    "    y_train = y[vfold_size:y.shape[0]]\n",
    "    return x_train, y_train, x_test, y_test, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d67a6495-5cf2-4833-b3aa-e9c06cf73661",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, class_names = load_data('./data')\n",
    "num_classes = len(class_names)\n",
    "image_size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb648456-3497-429c-9a38-6194cbfc13b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c48f32ec-cc58-4dd6-815b-041d06c57c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laptop\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "%matplotlib inline  \n",
    "idx = randint(0, len(x_train))\n",
    "plt.imshow(x_train[idx].reshape(28,28)) \n",
    "print(class_names[int(y_train[idx].item())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a55acaf6-92cf-4e48-b08d-37f5b03ba39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape and normalize\n",
    "x_train = x_train.reshape(x_train.shape[0], image_size, image_size, 1).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], image_size, image_size, 1).astype('float32')\n",
    "\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "# Convert class vectors to class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4a67367-5872-44fd-ac92-e4bdaac1c0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 14, 14, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 7, 7, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 3, 3, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 576)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               73856     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               12900     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,052\n",
      "Trainable params: 110,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Convolution2D(16, (3, 3),\n",
    "                        padding='same',\n",
    "                        input_shape=x_train.shape[1:], activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Convolution2D(32, (3, 3), padding='same', activation= 'relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Convolution2D(64, (3, 3), padding='same', activation= 'relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size =(2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='softmax')) \n",
    "# Train model\n",
    "# adam = tf.train.AdamOptimizer()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              # optimizer=adam,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['top_k_categorical_accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "644bb77e-7332-4d61-a8fb-dc9911f21f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1125/1125 - 92s - loss: 1.9067 - top_k_categorical_accuracy: 0.7844 - val_loss: 1.3940 - val_top_k_categorical_accuracy: 0.8741 - 92s/epoch - 81ms/step\n",
      "Epoch 2/5\n",
      "1125/1125 - 82s - loss: 1.2435 - top_k_categorical_accuracy: 0.8929 - val_loss: 1.1538 - val_top_k_categorical_accuracy: 0.9014 - 82s/epoch - 73ms/step\n",
      "Epoch 3/5\n",
      "1125/1125 - 85s - loss: 1.0796 - top_k_categorical_accuracy: 0.9105 - val_loss: 1.0733 - val_top_k_categorical_accuracy: 0.9124 - 85s/epoch - 75ms/step\n",
      "Epoch 4/5\n",
      "1125/1125 - 86s - loss: 0.9895 - top_k_categorical_accuracy: 0.9202 - val_loss: 0.9913 - val_top_k_categorical_accuracy: 0.9188 - 86s/epoch - 76ms/step\n",
      "Epoch 5/5\n",
      "1125/1125 - 86s - loss: 0.9284 - top_k_categorical_accuracy: 0.9260 - val_loss: 0.9503 - val_top_k_categorical_accuracy: 0.9232 - 86s/epoch - 76ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efbe2e4d250>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "model.fit(x = x_train, y = y_train, validation_split=0.1, batch_size = 256, verbose=2, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86bedc64-a8ff-49e7-a8c9-700592f3fa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuarcy: 92.32%\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test accuarcy: {:0.2f}%'.format(score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "73835af6-718a-4b3d-8fe1-88f43d36956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 112ms/step\n",
      "['hat', 'helmet', 'moustache', 'bread', 'laptop']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPcElEQVR4nO3dfZBV9X3H8c+XZYGwKgERJKglKsmIjkG7opW01WIUncygbWOliZGUFlvFh9RpY54a04cpMU2Ttj6kRKnkQW2maONMnFS6Y0tsIrIi8iAmoKIgFFSIrIan3f32jz2mG9zzu+u9595z4ft+zezcu+d7z54vl/3suff+zjk/c3cBOPwNKbsBAI1B2IEgCDsQBGEHgiDsQBBDG7mxYTbcR6itkZsEQtmrN7Xf99lAtZrCbmYzJf2DpBZJd7n7gtTjR6hNZ9uMWjYJIGG5d+TWqn4Zb2Ytkm6XdLGkKZJmm9mUan8egPqq5T37NEkb3f15d98v6X5Js4ppC0DRagn7REmb+32/JVv2S8xsnpl1mlnnAe2rYXMAalFL2Af6EOBtx966+0J3b3f39lYNr2FzAGpRS9i3SDq+3/fHSdpaWzsA6qWWsK+QNNnM3mtmwyRdIemhYtoCULSqh97cvdvM5kv6D/UNvS1y93WFdQagUDWNs7v7w5IeLqgXAHXE4bJAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EUdMsrhicllMmJ+tvnjw6WR+5+Y30Bp7bnFvq7epKr4swagq7mW2S1CWpR1K3u7cX0RSA4hWxZz/f3V8t4OcAqCPeswNB1Bp2l/SImT1pZvMGeoCZzTOzTjPrPKB9NW4OQLVqfRk/3d23mtk4SUvN7Fl3X9b/Ae6+UNJCSTrKxniN2wNQpZr27O6+NbvdIelBSdOKaApA8aoOu5m1mdmRb92XdKGktUU1BqBYtbyMHy/pQTN76+fc6+4/KKSrJmS/empu7eXPpd+dLD/rnmR95JBh1bQ0KMv2putzfvgHyfrk2w+kf8ATa95hRyhL1WF39+clfaDAXgDUEUNvQBCEHQiCsANBEHYgCMIOBMEprpkhU6ck619acndurVW9yXVPe+CTyfpxHemhu66J6f+mPcfmr99y6u7kusvO/8dkfcIFI5P133v+wmS968/ek198fHVyXRSLPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBGHujbt4zFE2xs+2GQ3bXn9DRoxI1i968n+T9akjXsqt3XrxZcl1e376XLJeppbx45L15685OVn/+se+nqx/YFj+ZbDPuv+m5Lonf3pFsu7d3cl6RMu9Q7t9pw1UY88OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0GEGWff/Llzk/VnrrkjWf/gdVfn1tqWLK+qp8PB0GPHJ+sv3HZMbu2Zc7+dXHfaUx9J1o/+nfypqiWpd2+F62gfhhhnB0DYgSgIOxAEYQeCIOxAEIQdCIKwA0GEGWc/feWAQ4+/sHN/W7K+5Zz887JRnS2fTh/7sGr+PyXrM9b+brL+rpmb8osN/L1vpJrG2c1skZntMLO1/ZaNMbOlZrYhux1dZMMAijeYl/H3SJp50LKbJXW4+2RJHdn3AJpYxbC7+zJJOw9aPEvS4uz+YkmXFtsWgKJV+wHdeHffJknZbe6FzMxsnpl1mlnnAe2rcnMAalX3T+PdfaG7t7t7e6uG13tzAHJUG/btZjZBkrLbHcW1BKAeqg37Q5Kuyu5fJel7xbQDoF4qzs9uZvdJOk/SWDPbIukLkhZI+q6ZzZX0kqT0iccNUOn65389/vvJ+mnfuj5ZP1E/fsc9Ie24v/1Rsn5G73XJ+tob0tcgOOUvrsmtnfDF9LYPRxXD7u6zc0rlHB0DoCocLgsEQdiBIAg7EARhB4Ig7EAQFT+NP1TsvODEZH24tSbrI7elT4FNnY55wm1rkuv2dnUl6xjYxC+lh8fO+vXLk/Uf/uGXc2tXLpmbXLd37bPJ+qGIPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBHHYjLNvn9Fd0/pPfyp9uuTrvXtya5d3zEv/8CfS4/Cozrhr8/9PJEmP5Zee/eQRyVXflx6GPySxZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIA6pcfbXP3pObu3Zi9LT+/Z4S7Le/jfzk/UJSzbmF7czjl6G7hc3J+uz1l2ZW3vh4ruS6+7a/PNkfWtP+voH6/Yfm6x//l9/P7c26fP1uWw5e3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCKKpxtlb3j0qWf/LL96dW3tqX/rv1u7e4cn6uDvS1yjvSVbRjLrvHZ9fPD297rmPX52s7331Xcn6ye/blqz/ZO6dubVpz/1Jct3R91Q3Dl9xz25mi8xsh5mt7bfsFjN72cxWZV+XVLV1AA0zmJfx90iaOcDyr7r71Ozr4WLbAlC0imF392WSdjagFwB1VMsHdPPNbHX2Mn903oPMbJ6ZdZpZ5wHtq2FzAGpRbdjvlHSSpKmStkn6St4D3X2hu7e7e3ur0h+SAaifqsLu7tvdvcfdeyV9Q9K0YtsCULSqwm5mE/p9e5mktXmPBdAcKo6zm9l9ks6TNNbMtkj6gqTzzGyqJJe0SVJ6UHKQXv7Eqcn6hSP/O7f22xsHGjD4f9dP7EjWf/bxX0vXJ+fXek5KX7/8nEmbkvVJI19L1lGd8a3VDxKNvXdksj7yweXJ+tDjj0tvILH6nmPS58rnfkBWQcWwu/vsARbnH90CoClxuCwQBGEHgiDsQBCEHQiCsANBNNUprrX49onfT9ZHDhmWrC9fkH/KYSWP/Lw1Wf/mjunJ+n9ufX/V20a+UcP35tZmH/VMct3huw7UtO0XP3pC1euOeqE+J1SzZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIJpqnH3civQ0uSm/tTp/ClxJahnSm6wPvW1sst72Pxtyaz27diXXlX6WrI6qUEfxPjb0N5P1lu6VyfquOelTon9wza3J+kXrr8ittf1b+vTZarFnB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgmmqcfchjq5L1kx/9RG5t4/n/klz3a7smJet3nZaeiLZtGZM2H2qGHHlkbu31S9KXLfc5ryTrPz799mT9uq0XJOtD5+TXupNrVo89OxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EYe7esI0dZWP8bJtRl5/92tz0+cXT/vipZP2OiY8n6+v250/L/OFHrk+ue/Ty9OEMozbtT9bVuP+it+lua0nW946uVM+ffnj/u9Pb7jnljWT941OeSNb/dMzq3FqleQQ69qT/XVf/+x8l65M/uypZ792bf037Wiz3Du32nQM+6RX37GZ2vJk9ambrzWydmd2QLR9jZkvNbEN2W+200QAaYDAv47sl3eTup0g6R9K1ZjZF0s2SOtx9sqSO7HsATapi2N19m7uvzO53SVovaaKkWZIWZw9bLOnSOvUIoADv6AM6M5sk6QxJyyWNd/dtUt8fBEnjctaZZ2adZtZ5QPtqbBdAtQYddjM7QtISSTe6++7BrufuC9293d3bWzW8mh4BFGBQYTezVvUF/Tvu/kC2eLuZTcjqEyTtqE+LAIpQcejNzEx978l3uvuN/ZZ/WdJr7r7AzG6WNMbd/zz1s+o59FarnvPPTNa3Xps/PLbs7H9Orju2pa2qnqL7rz3pfdFfvfDhZP3FVe/JrY1bkd72UQ+kLyXtByoMl5YkNfQ2mPPZp0u6UtIaM1uVLfuMpAWSvmtmcyW9JOkjBfQKoE4qht3dH5OUd2REc+6mAbwNh8sCQRB2IAjCDgRB2IEgCDsQxGFzimuZhowYkaz3nPn+ZP3Nien1yzR0T/r3Y/jO9CHQQ1/pyi9ufzW5bs/uQR+oiUxNp7gCODwQdiAIwg4EQdiBIAg7EARhB4Ig7EAQTTVl86Gq0mWB7UdPJ+tHFNlMk2Gi6+bBnh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCqBh2MzvezB41s/Vmts7MbsiW32JmL5vZquzrkvq3C6Bag7l4Rbekm9x9pZkdKelJM1ua1b7q7n9Xv/YAFGUw87Nvk7Qtu99lZuslTax3YwCK9Y7es5vZJElnSFqeLZpvZqvNbJGZjc5ZZ56ZdZpZ5wGlpwoCUD+DDruZHSFpiaQb3X23pDslnSRpqvr2/F8ZaD13X+ju7e7e3qrhtXcMoCqDCruZtaov6N9x9wckyd23u3uPu/dK+oakafVrE0CtBvNpvEm6W9J6d//7fssn9HvYZZLWFt8egKIM5tP46ZKulLTGzFZlyz4jabaZTZXkkjZJuroO/QEoyGA+jX9M0kDzPT9cfDsA6oUj6IAgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0GYuzduY2avSHqx36Kxkl5tWAPvTLP21qx9SfRWrSJ7+xV3P2agQkPD/raNm3W6e3tpDSQ0a2/N2pdEb9VqVG+8jAeCIOxAEGWHfWHJ209p1t6atS+J3qrVkN5Kfc8OoHHK3rMDaBDCDgRRStjNbKaZ/cTMNprZzWX0kMfMNpnZmmwa6s6Se1lkZjvMbG2/ZWPMbKmZbchuB5xjr6TemmIa78Q046U+d2VPf97w9+xm1iLpp5I+JGmLpBWSZrv7Mw1tJIeZbZLU7u6lH4BhZr8h6Q1J33T307Jlt0ra6e4Lsj+Uo939U03S2y2S3ih7Gu9stqIJ/acZl3SppDkq8blL9HW5GvC8lbFnnyZpo7s/7+77Jd0vaVYJfTQ9d18maedBi2dJWpzdX6y+X5aGy+mtKbj7Nndfmd3vkvTWNOOlPneJvhqijLBPlLS53/db1FzzvbukR8zsSTObV3YzAxjv7tukvl8eSeNK7udgFafxbqSDphlvmueumunPa1VG2AeaSqqZxv+mu/uZki6WdG32chWDM6hpvBtlgGnGm0K105/Xqoywb5F0fL/vj5O0tYQ+BuTuW7PbHZIeVPNNRb39rRl0s9sdJffzC800jfdA04yrCZ67Mqc/LyPsKyRNNrP3mtkwSVdIeqiEPt7GzNqyD05kZm2SLlTzTUX9kKSrsvtXSfpeib38kmaZxjtvmnGV/NyVPv25uzf8S9Il6vtE/jlJny2jh5y+TpT0dPa1ruzeJN2nvpd1B9T3imiupKMldUjakN2OaaLeviVpjaTV6gvWhJJ6+6D63hqulrQq+7qk7Ocu0VdDnjcOlwWC4Ag6IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQji/wDYg6ytqZJ6eAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inference\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "%matplotlib inline  \n",
    "idx = randint(0, len(x_test))\n",
    "img = x_test[idx]\n",
    "plt.imshow(img.squeeze()) \n",
    "pred = model.predict(np.expand_dims(img, axis=0))[0]\n",
    "ind = (-pred).argsort()[:5]\n",
    "latex = [class_names[x] for x in ind]\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f239937-f139-41dc-b0a8-0466c09e6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('modelled.txt', 'w') as file_handler:\n",
    "    for item in class_names:\n",
    "        file_handler.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce23733-8346-4dff-aadd-a3e86a961b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflowjs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c1a3747-dbdf-4100-baac-7f22062cc166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('keras.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce09fea8-79f6-4701-ada6-0dbaff7a38f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorflowjs_converter --input_format keras keras.h5 ./model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f5747864-5d2c-4f2e-8bc3-2a216fc5f78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp12iux06p/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp12iux06p/assets\n",
      "2022-07-04 20:22:53.248280: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-07-04 20:22:53.248333: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-07-04 20:22:53.249309: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmp12iux06p\n",
      "2022-07-04 20:22:53.252057: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-07-04 20:22:53.252086: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /tmp/tmp12iux06p\n",
      "2022-07-04 20:22:53.254507: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2022-07-04 20:22:53.260491: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-07-04 20:22:53.262138: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-07-04 20:22:53.340647: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmp12iux06p\n",
      "2022-07-04 20:22:53.362688: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 113382 microseconds.\n",
      "2022-07-04 20:22:53.413725: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-07-04 20:22:53.471893: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to TFlite for use with Flutter.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed80c30-e07d-489d-ba21-4a52ec5a4736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
